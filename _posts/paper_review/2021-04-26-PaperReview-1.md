---
title: "[Paper Review] Improving Calibration for Long-Tailed Recognition (ICLR 2021) ë…¼ë¬¸ ë¦¬ë·°"
excerpt: "Calibrationì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì— ëŒ€í•´ ì†Œê°œí•œë‹¤."

date: 2021-04-26
categories:
 - Paper_Review
tags:
  - paper_review
  - calibration
  - vision
  - deeplearning
layout: jupyter
search: true

# ëª©ì°¨
toc: true  
toc_sticky: true 

use_math: true
---

> âœğŸ» ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” ì ì€ ì´ë¯¸ì§€ë¡œ í•™ìŠµê°€ëŠ¥í•œ **Improving Calibration for Long-Tailed Recognition** ë…¼ë¬¸ ë¦¬ë·°ë¥¼ ê¼¼ê¼¼í•˜ê²Œ !!

- Paper : [Improving Calibration for Long-Tailed Recognition](https://arxiv.org/abs/2104.00466) (arxiv 2021 /Zhisheng Zhong, Jiequan Cui, Shu Liu, Jiaya Jia)

---

## 1. Abstract

ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ***Deep Neural Networks***ê°€ ***Training Datasets*ì´ ì‹¬í•œ *Class-Imbalance*ê°€ ìˆì„ ê²½ìš° ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆë‹¤**ê³  ë§í•˜ê³  ìˆë‹¤.<br><br> *Two-stage Method*ë¥¼ í†µí•´ *Representation Learning*ê³¼ *Classifier learning* ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸´ í–ˆì§€ë§Œ ì—¬ì „íˆ ***Miscalibration*ì´ ë°œìƒ**í•œë‹¤.<br><br> ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ë³¸ ë…¼ë¬¸ì—ì„œëŠ” 2ê°€ì§€ ë°©ë²•ì„ ì œì•ˆí•œë‹¤.
<br><br>
***"Motivated by the fact that predicted probability distributions of classes are highly related to the numbers of class instances, we propose label-aware smoothing to deal with different degrees of over-confidence for classes and improve classifier learning.<br>For dataset bias between these two stages due to different samplers, we further propose shifted batch normalization in the decoupling framework."***

---

## 2. Introduction
ë§ì´ ì“°ì´ëŠ” Open Dataset ê°™ì€ ê²½ìš°ì—ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ê°ê°ì˜ Object, Classì˜ Instance ìˆ˜ì™€ ê´€ë ¨í•´ì„œ ì¸ìœ„ì ìœ¼ë¡œ ê· í˜•ì„ ì´ë£¨ê³  ìˆë‹¤. <br><br>í•˜ì§€ë§Œ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ì¼ë°˜ì ì¸ ë°ì´í„°ì…‹ì€ ê°ê°ì˜ Classì˜ Instance ìˆ˜ê°€ ì‹¬ê°í•˜ê²Œ ë¶ˆê· í˜•í•œ ***Long-tailed Distribution***ì„ ë³´ì—¬ì£¼ê³  ìˆë‹¤. Long-tailed Distributionì•  ëŒ€í•´ CNNì„ í•™ìŠµì‹œí‚¬ ë•Œ ì„±ëŠ¥ì´ í¬ê²Œ ë–¨ì–´ì§„ë‹¤.
* * *
ì—¬ê¸°ì„œ ***Long-tailed Distribution*** ì´ë€?
<br><br>
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-1.png?raw=1" width = "800" ></p>

ì‰½ê²Œ ë§í•˜ë©´ **í´ë˜ìŠ¤ê°€ ê°€ì§€ê³  ìˆëŠ” ë°ì´í„° ì–‘ì˜ ì°¨ì´ê°€ í° ê²ƒ**ì„ ë§í•œë‹¤. <br><br>ì˜ˆë¥¼ ë“¤ë©´ ë³‘ì›ì—ì„œ ì§ˆë³‘ì´ ìˆëŠ” ì‚¬ëŒê³¼ ì§ˆë³‘ì´ ì—†ëŠ” ì‚¬ëŒì˜ ë°ì´í„°ë¥¼ ëª¨ì•„ì•¼ í•œë‹¤ê³  í–ˆì„ ë•Œ ì¼ë°˜ì ìœ¼ë¡œ ì§ˆë³‘ì´ ìˆëŠ” ì‚¬ëŒì˜ ë°ì´í„°ê°€ ì§ˆë³‘ì´ ì—†ëŠ” ì‚¬ëŒì˜ ë°ì´í„° ìˆ˜ì— ë¹„í•´ í˜„ì €í•˜ê²Œ ì ë‹¤. <br><br>ë¬¼ë¡  ë³‘ì› ë°ì´í„° ë¿ ì•„ë‹ˆë¼ í˜„ì‹¤ ë°ì´í„°ì—ì„œëŠ” ëŒ€ë¶€ë¶„ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆë‹¤. <br><br>ì´ëŸ¬í•œ í´ë˜ìŠ¤ ë¶ˆê· í˜•ìœ¼ë¡œ ì¸í•´ íŠ¹ì • í´ë˜ìŠ¤ì˜ Instanceê°€ ë„ˆë¬´ ë†’ê³  ë°˜ëŒ€ë¡œ ë‹¤ë¥¸ í´ë˜ìŠ¤ì˜ ê²½ìš°ëŠ” ë§¤ìš° ë‚®ê¸° ë•Œë¬¸ì— ë§ˆì¹˜ ê¸´ ê¼¬ë¦¬ ëª¨ì–‘ê³¼ ê°™ì´ ìƒê¸´ ê²ƒì„ ***Long-tailed Distribution***ì´ë¼ê³  ë§í•œë‹¤.
<br>
* * *
ë‹¤ì‹œ ë…¼ë¬¸ìœ¼ë¡œ ëŒì•„ì˜¤ë©´ ìµœê·¼ì—ëŠ” *Two-Stage Approach*ë¥¼ í†µí•´ì„œ ì„±ëŠ¥ì´ *One-stage Method*ì™€ ë¹„êµí–ˆì„ ë•Œ ìƒë‹¹íˆ ê°œì„ ë˜ì—ˆë‹¤.<br><br> *Two-Stage Approach*ì—ì„œ<br> ***Deffered Re-sampling(DRS)***ê³¼ ***Deffered Re-weighting(DRW)***ë°©ë²•ì´ ìˆë‹¤.
<br>
- 1. ì¼ë°˜ì ì¸ ë°©ë²•ìœ¼ë¡œ ë¶ˆê· í˜•ë˜ì–´ ìˆëŠ” Datasetì„ CNN Modelë¡œ í•™ìŠµì‹œí‚¨ë‹¤.
- 2. DRSë¡œ í´ë˜ìŠ¤ ê· í˜• ë¦¬ìƒ˜í”Œë§ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ì—ì„œ CNNì„ ì¡°ì •í•œë‹¤.
- 3. DRWë¡œ í´ë˜ìŠ¤ì— ë‹¤ë¥¸ weightë¥¼ í• ë‹¹í•¨ìœ¼ë¡œì¨ CNNì„ ì¡°ì •í•œë‹¤.

ë³¸ ë…¼ë¬¸ì—ì„œ ì°¸ê³ í•œ 2ê°€ì§€ ë…¼ë¬¸ê³¼ ë§í¬ëŠ” ì•„ë˜ì— ì²¨ë¶€í•˜ê² ìŠµë‹ˆë‹¤.
<br><br>
**_bbn: bilateral-branch network with cumulative learning for long-tailed visual recognition, CVPR 2020_** [Link](https://arxiv.org/abs/1912.02413 "ë…¼ë¬¸ ë§í¬")
<br><br>
**_Decoupling Representation and Classifier for Long-Tailed Recognition, ICLR 2020_** [Link](https://arxiv.org/abs/1910.09217 "ë…¼ë¬¸ ë§í¬")
<br><br>
ì²« ë²ˆì§¸ ë…¼ë¬¸ì—ì„œëŠ” ***Bilateral-Branch Network(BBN-Model)***ì„ ì œì•ˆí•œë‹¤.<br> ì´ ëª¨ë¸ì€ Representation learningê³¼ classifier learningì„ ë”°ë¡œ ìˆ˜í–‰í•˜ëŠ” í˜•íƒœì˜ í•™ìŠµ ë°©ë²•ì„ ì˜ë¯¸í•œë‹¤. 
<br><br>
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-2.png?raw=1" width = "800" ></p>
<br><br>
ìœ„ ëª¨ë¸ì€ 2ê°€ì§€ì˜ branchë¡œ ì´ë£¨ì–´ì ¸ ìˆë‹¤.<br>

### 1. Coventional learning branch
- Representation learning
- ì›ë˜ ***Long-tail distribution pattern*ì„ ê·¸ëŒ€ë¡œ í•™ìŠµí•˜ëŠ” ìš©ë„ë¡œ ì‚¬ìš©**ëœë‹¤.
- ***Typical uniform sampler*** ì‚¬ìš©<br><br>
ì´ ë•Œ ***Typical uniform sampler***ëŠ” í´ë˜ìŠ¤ ë¶ˆê· í˜•ì´ ìˆëŠ” Datasetì„ ê·¸ëŒ€ë¡œ sampling í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ë”°ë¼ì„œ ì´ ë•Œ í•™ìŠµí•  ë•Œ dataê°€ ë§ì€ class, ì¦‰ Head ìª½ Dataê°€ í•™ìŠµì´ ë” ë§ì´ ì§„í–‰ë˜ê²Œ ë˜ê³  ê²°ê³¼ì ìœ¼ë¡œ featureì˜ í•™ìŠµì¸ representation learningì´ ë” ì˜ ë˜ê²Œ ë§Œë“ ë‹¤.
<br><br>

### 2. Re-balancing branch
- Classifier learning
- Coventional learning branchì™€ ë‹¬ë¦¬ Tail ìª½ Dataë¥¼ ì¡°ê¸ˆ ë” ë§ì´ samplingí•œë‹¤. 
- Tail classì— ëŒ€í•œ classification accuracyë¥¼ ìƒìŠ¹ì‹œí‚¤ê¸° ìœ„í•œ ê²ƒì´ë‹¤.

<br><br>
ì´ ë…¼ë¬¸ìœ¼ë¡œë¶€í„° ì–»ì„ ìˆ˜ ìˆëŠ” ì •ë³´ëŠ” *Original Data*ë¡œ ë¶€í„° *Feature learning*ì˜ ì¥ì ì„ ì–»ê¸° ìœ„í•´<br><br> **Conventional branchë¥¼ í†µí•´ì„œ Original distributionì— ëŒ€í•´ í•™ìŠµ**ì„ ì§„í–‰í•œë‹¤.<br><br>
ì´ì „ ì‹¤í—˜ì—ì„œ *Representation learning*ì„ í•œ ì´í›„ì— *Classifier learning*ì„ *RW, RS* í˜•íƒœë¡œ ì§„í–‰í•œ ê²ƒì²˜ëŸ¼ *Alpha* ê°’ì„ ì¡°ì •í•˜ì—¬ ì²˜ìŒì—ëŠ” *uniform sampler*ë¡œë¶€í„° í•™ìŠµì„ ì‹œì‘í•˜ê³ ,<br> **ì´ê²ƒìœ¼ë¡œë¶€í„° featureê°€ ì˜ í•™ìŠµëœ Backbone ë„¤íŠ¸ì›Œí¬ë¡œë¶€í„° RS/RW íš¨ê³¼ë¥¼ ë‚´ëŠ” Re-balancing branchë¡œ ë¶€í„° í•™ìŠµì„ ëŠ˜ë¦°ë‹¤.** <br><br>
ìµœì¢…ì ìœ¼ë¡œ<br> ***Conventional learning branch***ëŠ” ***Majority Class***ì— preferenceë¥¼ ë” ê°€ì§€ë„ë¡, <br>***Re-balancing branch***ëŠ” ***Minority Class***ì— preferenceë¥¼ ë” ê°€ì§€ë„ë¡ í•™ìŠµì´ ëœë‹¤.

ë”°ë¼ì„œ Mix-upì„ í–ˆì„ ë•Œ, ì¦‰ ë‘ ê°€ì§€ ê²°ê³¼ë¥¼ í•©ì³¤ì„ ë•Œ ì´ëŸ° Weightê°€ Balanceí•œ í˜•íƒœë¡œ ê°€ì¥ ì˜ ë§ì¶°ì§€ê²Œ ëœë‹¤.<br><br>
ë‘ ë²ˆì§¸ ë…¼ë¬¸ì—ì„œëŠ” ***Two-stage decoupling Model***ì„ ì œì•ˆí•œë‹¤.<br> ì´ ëª¨ë¸ì€ ***classifier re-training(cRT)***ì™€ ***Learnable weight scaling(LWS)***ê°€ ìˆë‹¤.

#### 1. classifier re-training(cRT)
**Representation learning ë¶€ë¶„ì„ ê³ ì •**ì‹œí‚¤ê³  *Classifier*ë§Œ *Class Balanced* í˜•ì‹ìœ¼ë¡œ ë‹¤ì‹œ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì„ ì´ì•¼ê¸° í•œë‹¤.

#### 2. Learnable weight scaling(LWS)
*Scaling* í•˜ëŠ” ì •ë„ëŠ” í•™ìŠµì„ í†µí•´ì„œ ì–»ëŠ” ë°©ë²•ì„ ì˜ë¯¸í•œë‹¤.
<br><br>
***Confidence Calibration***<br>
*Calibration*ì´ë€ ëª¨í˜•ì˜ ì¶œë ¥ê°’ì´ ì‹¤ì œ *Calibrated Confidence*ë¥¼ ë°˜ì˜í•˜ë„ë¡ ë§Œë“œëŠ” ê²ƒì„ ë§í•œë‹¤. <br><br>
ì˜ˆë¥¼ ë“¤ì–´ Xì˜ Y1ì— ëŒ€í•œ ëª¨í˜•ì˜ ì¶œë ¥ì´ 0.8ì´ ë‚˜ì™”ì„ ë•Œ, 80% í™•ë¥ ë¡œ Y1ì¼ ê²ƒì´ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°–ë„ë¡ ë§Œë“œëŠ” ê²ƒì´ë‹¤. <br>ì¼ë°˜ì ìœ¼ë¡œ í˜„ëŒ€ ë”¥ëŸ¬ë‹ê°™ì€ ê²½ìš°ì—ëŠ” ***Overconfident*** ì„±ê²©ì„ ë„ê³  ìˆë‹¤. <br><br>ì˜ˆì‹œë¡œ ì•„ë˜ ê·¸ë¦¼ì„ ë³´ë©´ 1998ë…„ì— ì œì‹œëœ *LeNet*ì˜ ê²½ìš° ëª¨í˜•ì˜ ì¶œë ¥ì´ 0~1 ì‚¬ì´ì— ê· ì¼í•˜ê²Œ ë¶„í¬ë˜ì–´ ìˆì§€ë§Œ, *ResNet*ì˜ ê²½ìš° 1ê·¼ì²˜ì— ì§‘ì¤‘ë˜ì–´ ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. <br><br>ê·¸ ê²°ê³¼ë¡œ ì•„ë˜ ê·¸ë¦¼ì„ ë³´ê²Œ ë˜ë©´ *ResNet*ì˜ ê²½ìš° *Confidence*ì™€ *Accuracy*ê°€ ë§ì´ ì–´ê¸‹ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. <br>
**ëª¨í˜•ì˜ ì¶œë ¥ì´ ì‹¤ì œ *Calibrated Confidence*ë¥¼ ë°˜ì˜í•œë‹¤ë©´ *Confidence*ì™€ *Accuracy*ëŠ” ì¼ì¹˜í•´ì•¼ í•œë‹¤.**
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-3.png?raw=1" width = "800" ></p>
**ëª¨í˜•ì˜ ì˜ˆì¸¡ê°’ì´ ì‹¤ì œ í™•ë¥ ì„ ë°˜ì˜í•œë‹¤ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§„ *Calibration*ì´ ì¤‘ìš”í•œ ì´ìœ **<br><br>
ì‹¤ì œ ë”¥ëŸ¬ë‹ì´ ì‘ìš©ë  ë•Œ, ì˜ì‚¬ê²°ì • í”„ë¡œì„¸ìŠ¤ì¤‘ í•˜ë‚˜ì˜ êµ¬ì„±ìš”ì†Œê°€ ë  ê²½ìš°ê°€ ë§ë‹¤. <br><br>ì˜í•™ì  ì§„ë‹¨ì„ ì˜ˆë¡œ ë“¤ìë©´, ë”¥ëŸ¬ë‹ì„ ì „ì ìœ¼ë¡œ ì‹ ë¢°í•´ì„œ ëª¨ë“  íŒë‹¨ì„ ë”¥ëŸ¬ë‹ì—ê²Œ ë§¡ê¸°ëŠ” ì˜ì‚¬ê²°ì •ì´ ì´ë£¨ì–´ì§€ëŠ” ê²½ìš°ëŠ” ì ê³ , ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ *Confidence*ê°€ ë‚®ì€ ê²½ìš°ì—ë§Œ ì‚¬ëŒì´ ë³´ëŠ” ë°©ì‹ìœ¼ë¡œ ì‚¬ëŒì´ í•  ì¼ì˜ ì¼ë¶€ë¥¼ ë”¥ëŸ¬ë‹ì´ í•˜ê²Œ ë˜ëŠ” êµ¬ì¡°ê°€ ëŒ€ë¶€ë¶„ì…ë‹ˆë‹¤. <br><br>ì´ ê²½ìš° *Confidence*ê°€ ë‚®ì€ ê²ƒë§Œ ì‚¬ëŒì´ ì¬í™•ì¸í•˜ëŠ” ë°©ì‹ì´ ê°€ëŠ¥í•œë° ì´ëŸ¬í•œ ì˜ì‚¬ê²°ì •ì´ ê°€ëŠ¥í•˜ê¸° ìœ„í•´ì„œëŠ” ëª¨í˜•ì˜ *Confidence*ë¥¼ ë³´ëŠ” ê²ƒì´ í•„ìš”í•˜ê³  **ì´ *Confidence*ê°€ *Calibrated Confidence*ì´ì—¬ì•¼ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê°’**ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.<br><br>

ë„¤íŠ¸ì›Œí¬ì˜ *Calibration*ì„ ì¸¡ì •í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ***Expected Calibration Error(ECE)***ë¥¼ ì‚¬ìš©í•œë‹¤. <br><br> ***ECE***ëŠ” ***Confidence*ì™€ ì‹¤ì œ *Accuracy*ì˜ *Distribution*ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•**ì´ë‹¤. ì˜ˆì¸¡ê°’ì„ ê· ë“±í•˜ê²Œ Më¬¶ìŒìœ¼ë¡œ ë‚˜ëˆˆ ë’¤ì— *Accuracy*ì™€ *Confidence*ì°¨ì´ì˜ í‰ê· ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì—¬ê¸°ì„œ nì€ ìƒ˜í”Œì˜ ê°œìˆ˜ë‹¤.
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-4.png?raw=1" width = "800" ></p>

ì•„ë˜ ê·¸ë¦¼ì„ í†µí•´ í´ë˜ìŠ¤ì˜ ë¶ˆê· í˜• êµ¬ì„± ë¹„ìœ¨ ë•Œë¬¸ì— *Long-tailed Datasets*ì—ì„œ í›ˆë ¨ëœ ë„¤íŠ¸ì›Œí¬ê°€ ***Miscalibrated***í•˜ê³  ***Over-Confident***í•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤. <br>
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-5.png?raw=1" width = "800" ></p>

- ì›ë³¸ CIFAR-100 Dataset + CE(Cross Entropy)
- Long-tailed Datasets
- Long-tailed Datasets + cRT
- Long-tailed Datasets + LWS

ìœ„ ê²°ê³¼ê°’ì„ í†µí•´ *Long-tailed Datasets*ë¥¼ í›ˆë ¨í•œ *Network*ê°€ ì¼ë°˜ì ìœ¼ë¡œ ECEê°€ ë†’ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë˜í•œ *cRT, LWS* ì—ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ *Over-Confidence*ë¥¼ í™•ì¸í•  ìˆ˜ ìˆë‹¤. <br>ìœ„ í˜„ìƒì€ ë‹¤ë¥¸ *Long-tailed Datasets*ì—ì„œë„ ì¡´ì¬í•œë‹¤.
<br><br>
ë˜ ë‹¤ë¥¸ ë¬¸ì œëŠ”  *Two-stage Decoupling*ì´ *Dataset bias* ë˜ëŠ” *Domain shift*ë¥¼ ë¬´ì‹œí•œë‹¤ëŠ” ê²ƒì´ë‹¤.<br><br>
ì´ ë¬¸ì œëŠ” 1ë‹¨ê³„ì—ì„œ *Instanced balanced Dataset*ì— ëŒ€í•´ ë¨¼ì € í›ˆë ¨í•˜ê³  2ë‹¨ê³„ì—ì„œ ëª¨ë¸ì´ *Class-balanced dataset*ì—ì„œ í›ˆë ¨í–ˆì„ ë•Œ <br>
***Distribution of the dataset by different sampling ways*ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤.**<br><br>
ë”°ë¼ì„œ *Dataset bias* ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ Batch normalizationì— ì´ˆì ì„ ë‘”ë‹¤. <br><br>ìœ„ ë¬¸ì œë“¤ì„ ëª¨ë‘ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ë…¼ë¬¸ì—ì„œëŠ” ***Mixup Shifted Label-Aware Smoothing model(MiSLAS)***ë¥¼ ì œì•ˆí•œë‹¤.<br>
- *Long-tailed Dataset*ì— ëŒ€í•´ í›ˆë ¨ëœ *Model*ì´ *Balanced Dataset*ì— ëŒ€í•´ í›ˆë ¨ëœ *Model* ë³´ë‹¤ í›¨ì”¬ ***Miscalibrated and Over-confident*** 
<br>(ì´ëŠ” 2ë‹¨ê³„ ëª¨ë¸ ì—­ì‹œ ê°™ì€ ë¬¸ì œì  ë°œìƒ)
- *mixup*ì€ *representation learning*ì—ëŠ” ê¸ì •ì ì¸ ì˜í–¥ì„ ì£¼ê³  *Over-confidence*ë¥¼ ì¤„ì¼ ìˆ˜ ìˆì§€ë§Œ *Classifier learning*ì—ì„œëŠ” ë¶€ì •ì  ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆë‹¤.<br>**ë”°ë¼ì„œ *Classifier learning*ê³¼ *Calibration*ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ *Label-aware smoothing*ì„ ì œì•ˆí•œë‹¤.**<br><br>***Label-aware smoothing***ì´ë€? <br>***-->handle different degrees of over- confidence for classes***
- *Dataset bias or Domain shift*ë¥¼ *Decoupling Framework*ì—ì„œ í•´ê²°í•˜ê¸° ìœ„í•´ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆëŠ” ***Shift learning on the batch normalization layer***ë¥¼ ì œì•ˆí•œë‹¤.<br>
- *Long-tailed Dataset* ì—¬ëŸ¬ ê°œì—ì„œ MiSLASë¥¼ ê²€ì¦í•˜ê³  ì‹¤í—˜ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ë‹¤.

---

## 3. Related Work

***Re-sampling and Re-weighting***<br>
1. *Re-samling*
<br>
- ***Over-sampling the Tail-class images***<br>
  - *Over-sampling*ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì— ì •ê¸°ì ìœ¼ë¡œ ìœ ìš©
  - ì†Œê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ ***Tail Class*ì— ëŒ€í•œ *Over-fitting* ë°œìƒ**<br>
- ***Under-sampling the Head-class images***<br>
  - ë°ì´í„°ì˜ ë§ì€ ë¶€ë¶„ì„ íê¸°í•˜ë¯€ë¡œ ***Deep model*ì˜ ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì €í•˜**<br>

2. *Re-weighting*
<br>
- *Class & Instance*ì— ì„œë¡œ ë‹¤ë¥¸ *weight*ë¥¼ í• ë‹¹
- *Vanilla Re-weighting Method*
  - í´ë˜ìŠ¤ ìƒ˜í”Œ ìˆ˜ì— **ì—­ë¹„ë¡€**í•˜ì—¬ ***Class weight*ë¥¼ ì œê³µ**
- ëŒ€ê·œëª¨ ë°ì´í„°ì˜ ê²½ìš° í•™ìŠµì‹œí‚¤ëŠ” ë™ì•ˆ *Deep Model*ì„ ìµœì í™”í•˜ê¸° ì–´ë ¤ì›€
  - ìœ íš¨ ìˆ«ìë¥¼ ì‚¬ìš©í•˜ì—¬ *Class weight*ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ„ ë¬¸ì œ í•´ê²°
  - ê° ì¸ìŠ¤í„´ìŠ¤ì˜ *weight*ë¥¼ ì ì‘ì ìœ¼ë¡œ ë‹¤ì‹œ ë§¤ê¹€ <br>(ex.***Focal loss*** -> ì˜ ë¶„ë¥˜ëœ ì˜ˆì œì—ëŠ” ì‘ì€ *weight*, ë¶„ë¥˜í•˜ê¸° ì–´ë ¤ìš´ ì¼ë¶€ ì˜ˆì œì—ëŠ” í° *weight*ë¥¼ ë¶€ì—¬í•˜ì—¬ í•™ìŠµì„ ì–´ë ¤ìš´ ì˜ˆì œì— ì§‘ì¤‘ì‹œí‚´)

***Confidence calibration and regularization***<br>
- *Calibrated confidence*ëŠ” *Classification model*ì—ì„œ ì¤‘ìš”<br>
  - *Model capacity, Normalization, Regularization -> Network Calibration*ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì„ í™•ì¸
- *Mixup*<br>*Interpolation of input and labels*ìœ¼ë¡œ í›ˆë ¨í•˜ëŠ” *Regularization* ê¸°ë²•
    - *manifold mixup, Cut- Mix, Remix*
    - *Mixup*ìœ¼ë¡œ í•™ìŠµëœ *CNN* -> ***Better calibrated*** 
- *Label smoothing*<br>*another Regularization* ê¸°ë²• <br>-->**Over-confident**ë¥¼ ì¤„ì´ë„ë¡ *Model*ì„ ë§Œë“ ë‹¤.
    - *compute loss upon a soft version of labels*
    - *relieve Over-fitting and increase Calibration and Reliability*

***Mixup***ì€ í•™ìŠµì„ ì§„í–‰í•  ë•Œ ëœë¤í•˜ê²Œ ë‘ ê°œì˜ ìƒ˜í”Œ (x(i),y(i)), (x(j),y(j))ë¥¼ ë½‘ì•„ì„œ (x_dot,y_dot)ì„ ë§Œë“¤ì–´ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.<br>
ì•„ë˜ ê·¸ë¦¼ì€ ***Mixup***ì— ëŒ€í•œ ì˜ˆì‹œë¥¼ ë³´ì—¬ì¤€ë‹¤. *lambda*ëŠ” ë³´í†µ 0.5ê°€ ì•„ë‹Œ í•œ ìª½ ë°ì´í„°ì— ì¹˜ìš°ì¹˜ë„ë¡ 0.1 í˜¹ì€ 0.2ì •ë„ë¥¼ ì¤€ë‹¤.
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-8.png?raw=1" width = "800" ></p>
<br><br>
***Label smoothing***ì€ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©í•˜ëŠ” ê¸°ë²•ì´ë‹¤.<br> *one-hot encoding*ì²˜ëŸ¼ ì •ë‹µ ë ˆì´ë¸”ì— 1 ì•„ë‹Œ ë ˆì´ë¸”ì— 0ì„ ë¶€ì—¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ <br>ì •ë‹µ ë ˆì´ë¸”ì´ ì•„ë‹Œ ë ˆì´ë¸”ì—ë„ ì•½ê°„ì˜ ë ˆì´ë¸” ê°’ì„ ë„£ì–´ì£¼ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤.<br>
ì•„ë˜ ê·¸ë¦¼ì€ *Label smoothing*ì— ëŒ€í•œ ì˜ˆì‹œì´ë‹¤.<br><br>
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-9.png?raw=1" width = "800" ></p>

***Two-stage methods***<br>
- ***Deffered Re-weighting(DRW) & Deffered Re-sampling(DRS)***<br>
*better than conventional one-stage methods*
  - ë” ë‚˜ì€ Featureì—ì„œ ì‹œì‘í•´ì„œ ***adjust the decision boundary and locally tunes features***
- ***Decomposing representation and classifier learning***<br>
  - **ë¨¼ì € *Instance-balanced sampling*ìœ¼ë¡œ *Deep Model*ì„ í•™ìŠµ**
    - ê·¸ í›„ *Parameters of Representation learning*ì´ ê³ ì •ëœ *Class-balanced sampling*ìœ¼ë¡œ *classifier*ë¥¼ ë¯¸ì„¸ ì¡°ì •
  - ***The cumulative learning strategy***<br>
    - *bridge the representation learning and classifier re-balancing*
    - *requires dual samplers of instance-balanced and reversed instance- balanced sampler*

---

### 3.1 Study of mixup Strategy

***Instance-balanced sampling & mixup***<br>
*Instance-balanced sampling : **The most general representation among all for long-tailed recognition***<br>
*mixup : **The Network trained with mixup are better calibrated***<br>

- ***Mixup in the Two-stage Decoupling framework***
  - *Higher representation generalization*
  - *reduce Over-confidence*
<br><br>

***Stage 1***<br>
180epochs ë™ì•ˆ *ImageNet-LT*ì—ì„œ *Original Cross-entropy Model, Two stage Models of cRT and LWS* í•™ìŠµì‹œí‚¨ë‹¤ <br><br>
***Stage 2***<br>
ê°ê° 10epochsì— ëŒ€í•´ ë¯¸ì„¸ ì¡°ì •í•œë‹¤.
<br><br>
ë‘ ë‹¨ê³„ì— ëŒ€í•œ *Training setup(with/without mixup alpha = 0.2)*ì„ ë³€ê²½í•œë‹¤. <br><br>*ì²´í¬í‘œì‹œëŠ” mixupì´ ì ìš©í–ˆì„ ë•Œ xí‘œì‹œëŠ” mixupì„ ì ìš©í•˜ì§€ ì•Šì„ ë•Œì´ë‹¤.*<br>(Top-1 Accuracy / ECEì— ëŒ€í•œ í‘œ)
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-6.png?raw=1" width = "800" ></p>

- *Mixup*ì„ ì ìš©í–ˆì„ ë•Œ<br>
  - *Improvement of Cross Entropy*ëŠ” ***can be ignored***
  - *Stage 1*ì—ì„œ *cRT, LWS* ëª¨ë‘ **ì„±ëŠ¥ì´ í¬ê²Œ í–¥ìƒ**
  - *Stage 2*ì—ì„œ ì¶”ê°€ë¡œ *mixup*ì„ ì§„í–‰ì‹œ **ê°œì„  íš¨ê³¼ê°€ ì—†ê±°ë‚˜ ì˜¤íˆë ¤ ì„±ëŠ¥ì„ ì†ìƒì‹œí‚´**
  
**ìœ„ ê²°ê³¼ì— ëŒ€í•œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ëŠ” mixupì˜ ì—­í• ** 
<br>1. *encourages representation learning*
<br>2. *but, adverse or negligible effect on classifier learning*<br><br>
**ì •ë¦¬**<br>
ì¦‰, 1ë‹¨ê³„ì—ì„œ *mixup*ì€ representation learningì—ëŠ” ì¢‹ì€ íš¨ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ë§Œ <br>**2ë‹¨ê³„ì—ì„œëŠ” *classifier learning*ì—ì„œëŠ” íš¨ê³¼ë¥¼ ë³´ì—¬ì£¼ì§€ ëª»í•˜ê±°ë‚˜ ì˜¤íˆë ¤ ì•…ì˜í–¥ì„ ë¯¸ì¹œë‹¤.**<br>
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-7.png?raw=1" width = "800" ></p>
ìœ„ ê·¸ë¦¼ì€ ***Final classifier weight norms***ì„ í™•ì¸í•œ ê²ƒì´ë‹¤. ìœ„ ê·¸ë¦¼ì„ ë´¤ì„ ë•Œ *mixup*ì´ *tail classes*ì— ë” ìš°í˜¸ì ì¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.<br><br>

ê·¸ë˜ì„œ 2ë‹¨ê³„ì—ì„œ *mixup*ì„ ì¶”ê°€í–ˆì„ ë•Œ ìƒê¸°ëŠ” ë¶ˆì•ˆì •í•œ ê²°ê³¼ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ì„ ì¶”ê°€ë¡œ ì œì•ˆí•œë‹¤.<br>
***-> Label-aware smoothing***

### 3.2. Label-aware smoothing

*Cross-entropy*ì˜ ìµœì ì˜ ì†”ë£¨ì…˜ê³¼ ë¹„êµí•˜ì—¬,<br><br>***Label-aware smoothing***<br>
- ***encourage a finite output, more general and remedying overfit***

<br><br>
ë˜í•œ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ê°€ ë” ë§ì€ í´ë˜ìŠ¤ì¸ Head classê°€ 
<br>ë” ë‹¤ì–‘í•œ ì˜ˆë¥¼ ë§ì´ í¬í•¨í•˜ê³  ìˆê¸° ë•Œë¬¸ì— ì˜ˆì¸¡ í™•ë¥ ì´ *Tail class*ë³´ë‹¤ ë” ì¢‹ë‹¤.<br>
ë”°ë¼ì„œ ë” í° ***Label smoothing factor***ë¥¼ ë¶€ì—¬í•´ì•¼ í•œë‹¤ê³  ë…¼ë¬¸ì—ì„œëŠ” ë§í•˜ê³  ìˆë‹¤.<br><br>

ê·¸ë¦¬ê³  *Label-aware smoothing*ì€ *Cross-entropy*ë³´ë‹¤ ë” ë³µì¡í•˜ê¸° ë•Œë¬¸ì—
<br>***Generalized classifier learning framework***ì— ì ìš©í•´ì•¼ í•œë‹¤ê³  ë§í•˜ê³  ìˆê³ <br>
ì˜ˆì‹œë¡œëŠ” ìœ„ì—ì„œ ë°°ìš´ ***cRT*** í˜¹ì€ ***LWS***ë¥¼ ë§í•œë‹¤.<br><br>

*cRT*ì™€ *LWS*ì¤‘ì—ì„œ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ *LWS*ê°€ ë” ì¢‹ì€ ê²°ê³¼ë¥¼ ì£¼ê¸° ë•Œë¬¸ì—<br>
ì‹¤í—˜ì—ì„œëŠ” ***LWS + Label-aware smoothing***ìœ¼ë¡œ ê²°ê³¼ë¥¼ í™•ì¸í•œë‹¤. <br>
ê²°ê³¼ëŠ” ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ë‹¤.
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-10.png?raw=1" width = "800" ></p>
ì™¼ìª½ë¶€í„° *Head, Medium,Tail* ìˆœìœ¼ë¡œ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤€ ê²ƒì´ë‹¤.<br>
(ì—°í•œ íŒŒë‘ : *LWS + Cross-Entropy* , ì§™ì€ íŒŒë‘ : *LWS + Label-Aware Smoothing*)
<br><br>
***LWS + Cross-Entropy***ì˜ ê²½ìš°ì—” <br>*Head*ì™€ *Medium*ì—ì„œ ì‹¤ì œë¡œ 1.0ì— ê°€ê¹Œìš¸ ì •ë„ë¡œ ë†’ì€ *Over-confident*ë¥¼ ë³´ì´ëŠ” ë°˜ë©´ì—,<br><br>
***LWS + Label-Aware Smoothing***ì˜ ê²½ìš°ì—” <br> *Over-confident*ê°€ ë§ì´ ê°ì†Œí•œ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.<br><br>

### 3.3. Shift Learning on Batch Normalization

ì¸ìŠ¤í„´ìŠ¤ ê· í˜• ìƒ˜í”Œë§ìœ¼ë¡œ 1ë‹¨ê³„ì—ì„œ í•™ìŠµí•œ í›„ í´ë˜ìŠ¤ ê· í˜• ìƒ˜í”Œë§ìœ¼ë¡œ 2ë‹¨ê³„ì—ì„œ í•™ìŠµí•œë‹¤.<br><br>
ìœ„ *Two-stage training framework*ëŠ” ***Transfer learning*ì˜ ë³€í˜•**ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆëŠ”ë°<br><br>
Transfer learning ê´€ì ì—ì„œ Two-stage training frameworkë¥¼ ë³´ë©´<br><br> backboneë¶€ë¶„ì„ ê³ ì •í•˜ê³  Classifierë¥¼ íŠœë‹í•˜ëŠ” ê²ƒì€ unreasonable í•˜ë‹¤. <br><br>
ë‹¤ë¥¸ ìƒ˜í”Œë§ ë°©ë²•ì´ê¸° ë•Œë¬¸ì— *Head, Medium, Tail* êµ¬ì„± ë¹„ìœ¨ì´ ë‹¤ë¥´ê³ , ë”°ë¼ì„œ *Bias*ê°€ ì¡´ì¬í•œë‹¤.<br><br>
2ê°€ì§€ ë°©ë²•ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©<br>
- ***AdaBN & TransNorm***
  - update the running mean Î¼ and variance Ïƒ
  - fix the learnable linear transformation parameters Î± and Î² for better normalization in Stage-2

---

## 4. Experiments

### 4.1.1 Datasets and Setup

***1. CIFAR-10 and CIFAR-100***<br>
50000ì¥ Training & 10000ì¥ Validation + 10ê°œ ì¹´í…Œê³ ë¦¬ í˜¹ì€ 100ê°œ ì¹´í…Œê³ ë¦¬<br>
***Long-tailed Dataset ì‚¬ìš©***<br><br>

***2. ImageNet-LT and Places-LT***<br>
- ImageNet-LT<br>
115800 ì´ë¯¸ì§€ + 100 ì¹´í…Œê³ ë¦¬ (class cardinality:5~1280)
- Places-LT<br>
184500 ì´ë¯¸ì§€ + 365 ì¹´í…Œê³ ë¦¬ (class cardinality:5~4980)

***3. iNaturalist 2018***<br>
437500 ì´ë¯¸ì§€ + 8142 ì¹´í…Œê³ ë¦¬

### 4.1.2 Implementation Details

***SGD optimizer with momentum = 0.9 to optimize network***<br>
- MiSLAS model with ResNet-32 + 160~180 epochsì—ì„œ 0.1ë¡œ learning rate ê°ì†Œ<br>
- Use cosine learning rate -> MiSLAS model + ResNet- 10, 50, 101, 152

### 4.2 Ablation Study

***Calibration performance***
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-11.png?raw=1" width = "800" ></p>
CIFAR-100-LT with IF 100 ë°ì´í„°ì…‹ìœ¼ë¡œ í–ˆì„ ë•Œ <br>Calibration performanceì— ëŒ€í•œ ê²°ê³¼ì´ë‹¤.<br>
**ë³¸ ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” MiSLAS ëª¨ë¸ì¼ ë•Œê°€ ê°€ì¥ Confidence gapì´ ì ì€ ê²ƒì„ ë³´ì—¬ì¤€ë‹¤.**<br><br>
***Comparing re-weighting with label-aware smoothing***
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-12.png?raw=1" width = "800" ></p>
class balanced cross-entropyì™€ Label-aware smoothingì„ ë¹„êµí–ˆì„ ë•Œ ê²°ê³¼ì´ë‹¤.<br>
ìœ„ ê²°ê³¼ì—ì„œ ì•Œ ìˆ˜ ìˆë“¯ì´ Label-aware smoothingì„ í–ˆì„ ë•Œ <br>***Over-confidence*ë„ í¬ê²Œ ê°ì†Œ**í•˜ê³  ***Accuracy*ë„ ìƒìŠ¹**í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

### 4.2.1 Result

<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-13.png?raw=1" width = "800" ></p>
**ìœ„ ê²°ê³¼ í‘œë¥¼ í†µí•´ ì•Œ ìˆ˜ ìˆëŠ” ì **
- 1ë‹¨ê³„ì—ì„œ mixupì„ í–ˆì„ ë•Œ Accuracyì¦ê°€ + ECE ê°ì†Œ
- Shift learning on BN + Label-aware smoothingê¹Œì§€ í–ˆì„ ë•Œ Accuracy ì•½ê°„ ì¦ê°€ + ECE í¬ê²Œ ê°ì†Œ

<br>
***Comparison with State-of-the-arts***
<p align="center"><img src="https://github.com/Changhyun-song/Changhyun-song.github.io/blob/main/_posts/images/paper_review/paper_review1/paper_review1-14.png?raw=1" width = "800" ></p>
<br><br>
ì „ì²´ì ìœ¼ë¡œ ë³¸ ë…¼ë¬¸ ì´ì „ì— ì‚¬ìš©ë˜ì—ˆë˜ ë°©ë²•ë“¤ì´ë‘ ë¹„êµí–ˆì„ ë•Œ <br>***MiSLASê°€ ì••ë„ì ìœ¼ë¡œ ë†’ì€ Accuracy + ì¢‹ì€ Calibrationì„ì„ ë³´ì—¬ì¤€ë‹¤.***
<br>
ëŒ€ê·œëª¨ ë°ì´í„° ì…‹ì¸ a,b,cì—ì„œë„ MiSLASê°€ ë†’ì€ ì„±ëŠ¥ì„ ê°€ì§€ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤.

---

### 5 Conclusion

1. ***Long-tailed Datasetì„ í•™ìŠµí•œ ëª¨ë¸ì€*** <br>balanced datasetì„ í•™ìŠµí•œ ëª¨ë¸ë³´ë‹¤ ***miscalibrated and overconfident***
2. ì²« ë²ˆì§¸ ì†”ë£¨ì…˜ - ***Mixup***
- 1ë‹¨ê³„ì—ì„œ mixup ì‚¬ìš© -> ***representation learningì—ì„œ ì¢‹ì€ íš¨ê³¼***(classifier learningì—ì„œëŠ” ì˜¤íˆë ¤ ì—­íš¨ê³¼)
3. ë‘ ë²ˆì§¸ ì†”ë£¨ì…˜ - ***Label-aware smoothing***
- LWSë¥¼ ì‚¬ìš©í•˜ì—¬ Over-confidenceë¥¼ í¬ê²Œ ê°ì†Œì‹œí‚¨ë‹¤.
4. ì„¸ ë²ˆì§¸ ì†”ë£¨ì…˜ - ***Shift learning on the batch normaization***
- Two-stage method frameworkì—ì„œ ***Dataset biasë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œ*** ì‚¬ìš© -> ì„±ëŠ¥ í–¥ìƒ
5. ë…¼ë¬¸ì—ì„œ ì œì‹œí•˜ëŠ” ***MiSLAS ëª¨ë¸ì´ ê°€ì¥ ì¢‹ì€ Accuracy + Calibration*** ì„ ë³´ì—¬ì¤Œ
- ëŒ€ê·œëª¨ ë°ì´í„° ì…‹ì—ì„œë„ ë§ˆì°¬ê°€ì§€ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ